{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IxevrpjX7he"
      },
      "source": [
        "# EBAC - Regressão II - regressão múltipla\n",
        "\n",
        "## Tarefa I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjryA0_CX7hg"
      },
      "source": [
        "#### Previsão de renda II\n",
        "\n",
        "Vamos continuar trabalhando com a base 'previsao_de_renda.csv', que é a base do seu próximo projeto. Vamos usar os recursos que vimos até aqui nesta base.\n",
        "\n",
        "|variavel|descrição|\n",
        "|-|-|\n",
        "|data_ref                | Data de referência de coleta das variáveis |\n",
        "|index                   | Código de identificação do cliente|\n",
        "|sexo                    | Sexo do cliente|\n",
        "|posse_de_veiculo        | Indica se o cliente possui veículo|\n",
        "|posse_de_imovel         | Indica se o cliente possui imóvel|\n",
        "|qtd_filhos              | Quantidade de filhos do cliente|\n",
        "|tipo_renda              | Tipo de renda do cliente|\n",
        "|educacao                | Grau de instrução do cliente|\n",
        "|estado_civil            | Estado civil do cliente|\n",
        "|tipo_residencia         | Tipo de residência do cliente (própria, alugada etc)|\n",
        "|idade                   | Idade do cliente|\n",
        "|tempo_emprego           | Tempo no emprego atual|\n",
        "|qt_pessoas_residencia   | Quantidade de pessoas que moram na residência|\n",
        "|renda                   | Renda em reais|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "VA0Aj55hX7hh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Lasso\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.tree import DecisionTreeRegressor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "VKhtoqyXX7hh"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('previsao_de_renda.csv')\n",
        "data = pd.DataFrame(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hIGzr0UX7hh",
        "outputId": "9ad304c6-19a3-4514-fd9e-b1b453a2e497"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15000 entries, 0 to 14999\n",
            "Data columns (total 15 columns):\n",
            " #   Column                 Non-Null Count  Dtype  \n",
            "---  ------                 --------------  -----  \n",
            " 0   Unnamed: 0             15000 non-null  int64  \n",
            " 1   data_ref               15000 non-null  object \n",
            " 2   id_cliente             15000 non-null  int64  \n",
            " 3   sexo                   15000 non-null  object \n",
            " 4   posse_de_veiculo       15000 non-null  bool   \n",
            " 5   posse_de_imovel        15000 non-null  bool   \n",
            " 6   qtd_filhos             15000 non-null  int64  \n",
            " 7   tipo_renda             15000 non-null  object \n",
            " 8   educacao               15000 non-null  object \n",
            " 9   estado_civil           15000 non-null  object \n",
            " 10  tipo_residencia        15000 non-null  object \n",
            " 11  idade                  15000 non-null  int64  \n",
            " 12  tempo_emprego          12427 non-null  float64\n",
            " 13  qt_pessoas_residencia  15000 non-null  float64\n",
            " 14  renda                  15000 non-null  float64\n",
            "dtypes: bool(2), float64(3), int64(4), object(6)\n",
            "memory usage: 1.5+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3jAB1OUX7hi"
      },
      "source": [
        "1. Separe a base em treinamento e teste (25% para teste, 75% para treinamento).\n",
        "2. Rode uma regularização *ridge* com alpha = [0, 0.001, 0.005, 0.01, 0.05, 0.1] e avalie o $R^2$ na base de testes. Qual o melhor modelo?\n",
        "3. Faça o mesmo que no passo 2, com uma regressão *LASSO*. Qual método chega a um melhor resultado?\n",
        "4. Rode um modelo *stepwise*. Avalie o $R^2$ na vase de testes. Qual o melhor resultado?\n",
        "5. Compare os parâmetros e avalie eventuais diferenças. Qual modelo você acha o melhor de todos?\n",
        "6. Partindo dos modelos que você ajustou, tente melhorar o $R^2$ na base de testes. Use a criatividade, veja se consegue inserir alguma transformação ou combinação de variáveis.\n",
        "7. Ajuste uma árvore de regressão e veja se consegue um $R^2$ melhor com ela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fStgJ-WX7hi",
        "outputId": "32c20879-b709-48ae-c924-6a469451251d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do conjunto de treinamento: (11250, 14)\n",
            "Tamanho do conjunto de teste: (3750, 14)\n"
          ]
        }
      ],
      "source": [
        "#1\n",
        "\n",
        "# Garantir que X e y sejam numéricos\n",
        "X = X.apply(pd.to_numeric, errors='coerce')\n",
        "y = pd.to_numeric(y, errors='coerce')\n",
        "\n",
        "# Variáveis independentes (X) e dependente (y)\n",
        "X = df.drop('renda', axis=1)\n",
        "y = df['renda']\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste (75% para treinamento, 25% para teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "# Verificar o tamanho dos conjuntos\n",
        "print(\"Tamanho do conjunto de treinamento:\", X_train.shape)\n",
        "print(\"Tamanho do conjunto de teste:\", X_test.shape)"
      ]
    },
    {
      "source": [
        "#2\n",
        "\n",
        "# Preencher valores nulos\n",
        "df['tempo_emprego'].fillna(df['tempo_emprego'].median(), inplace=True)\n",
        "\n",
        "# Codificação das variáveis categóricas\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Dividir os dados em variáveis independentes (X) e dependente (y)\n",
        "X = df.drop('renda', axis=1)\n",
        "y = df['renda']\n",
        "\n",
        "# Dividir os dados em conjuntos de treinamento e teste (75% treinamento, 25% teste)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Valores de alpha para testar\n",
        "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "r2_scores = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    # Criar e treinar o modelo Ridge\n",
        "    model = Ridge(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer previsões na base de teste\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Avaliar o modelo com o R²\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "# Exibir os resultados\n",
        "for alpha, r2 in zip(alphas, r2_scores):\n",
        "    print(f'Alpha: {alpha}, R² Score: {r2}')\n",
        "\n",
        "# Encontrar o melhor alpha com base no R²\n",
        "best_alpha = alphas[np.argmax(r2_scores)]\n",
        "best_r2 = max(r2_scores)\n",
        "\n",
        "print(f'Melhor alpha: {best_alpha} com R² Score: {best_r2}')"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl6i_NaAbB-x",
        "outputId": "c6229518-11a6-4953-db4e-1a0a979b4559"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0, R² Score: 0.26802204227884163\n",
            "Alpha: 0.001, R² Score: 0.26802204633430404\n",
            "Alpha: 0.005, R² Score: 0.2680220609254066\n",
            "Alpha: 0.01, R² Score: 0.2680220755752635\n",
            "Alpha: 0.05, R² Score: 0.2680220637071694\n",
            "Alpha: 0.1, R² Score: 0.26802178629490303\n",
            "Melhor alpha: 0.01 com R² Score: 0.2680220755752635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-97-c74c07a90982>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['tempo_emprego'].fillna(df['tempo_emprego'].median(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "\n",
        "# Valores de alpha para testar\n",
        "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
        "r2_scores_lasso = []\n",
        "\n",
        "for alpha in alphas:\n",
        "    # Criar e treinar o modelo LASSO\n",
        "    model = Lasso(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Fazer previsões na base de teste\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Avaliar o modelo com o R²\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    r2_scores_lasso.append(r2)\n",
        "\n",
        "# Exibir os resultados\n",
        "for alpha, r2 in zip(alphas, r2_scores_lasso):\n",
        "    print(f'Alpha: {alpha}, R² Score: {r2}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amYyEa-gej-C",
        "outputId": "f55feaf8-2b16-458d-841a-735acec67ec5"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0, R² Score: 0.26802029222505375\n",
            "Alpha: 0.001, R² Score: 0.26802027212645974\n",
            "Alpha: 0.005, R² Score: 0.2680201579239637\n",
            "Alpha: 0.01, R² Score: 0.2680199389911513\n",
            "Alpha: 0.05, R² Score: 0.2680151506637477\n",
            "Alpha: 0.1, R² Score: 0.2680016177634079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.854e+11, tolerance: 7.723e+07\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultados da regularização Ridge\n",
        "r2_scores_ridge = [0.26802204227884163, 0.26802204633430404, 0.2680220609254066, 0.2680220755752635, 0.2680220637071694, 0.26802178629490303]\n",
        "\n",
        "# Comparar os melhores modelos\n",
        "best_alpha_ridge = alphas[np.argmax(r2_scores_ridge)]\n",
        "best_r2_ridge = max(r2_scores_ridge)\n",
        "\n",
        "best_alpha_lasso = alphas[np.argmax(r2_scores_lasso)]\n",
        "best_r2_lasso = max(r2_scores_lasso)\n",
        "\n",
        "print(f'Melhor alpha Ridge: {best_alpha_ridge} com R² Score: {best_r2_ridge}')\n",
        "print(f'Melhor alpha LASSO: {best_alpha_lasso} com R² Score: {best_r2_lasso}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtHXEUJ9fSxD",
        "outputId": "9112080c-e869-410c-c1d6-d851a08032bd"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Melhor alpha Ridge: 0.01 com R² Score: 0.2680220755752635\n",
            "Melhor alpha LASSO: 0 com R² Score: 0.26802029222505375\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#4\n",
        "\n",
        "def stepwise_selection(X, y,\n",
        "                       initial_list=[],\n",
        "                       threshold_in=0.01,\n",
        "                       threshold_out=0.05,\n",
        "                       verbose=True):\n",
        "    \"\"\"\n",
        "    Performs stepwise regression to select features.\n",
        "\n",
        "    Args:\n",
        "        X (pd.DataFrame): The independent variables.\n",
        "        y (pd.Series): The dependent variable.\n",
        "        initial_list (list): An initial list of features to include.\n",
        "        threshold_in (float): The p-value threshold for adding a feature.\n",
        "        threshold_out (float): The p-value threshold for removing a feature.\n",
        "        verbose (bool): Whether to print the steps of the selection.\n",
        "\n",
        "    Returns:\n",
        "        list: The list of selected features.\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure all columns in X are numeric\n",
        "    # This is crucial for statsmodels compatibility\n",
        "    X = X.select_dtypes(include=np.number)\n",
        "\n",
        "    included = list(initial_list)\n",
        "    while True:\n",
        "        changed = False\n",
        "        # Forward step\n",
        "        excluded = list(set(X.columns) - set(included))\n",
        "        new_pval = pd.Series(index=excluded)\n",
        "        for new_column in excluded:\n",
        "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included + [new_column]]))).fit()\n",
        "            new_pval[new_column] = model.pvalues[new_column]\n",
        "        best_pval = new_pval.min()\n",
        "        if best_pval < threshold_in:\n",
        "            best_feature = new_pval.idxmin()\n",
        "            included.append(best_feature)\n",
        "            changed = True\n",
        "            if verbose:\n",
        "                print(f'Add  {best_feature:30} with p-value {best_pval:.6}')\n",
        "\n",
        "        # Backward step\n",
        "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
        "        # Use all coefs except intercept\n",
        "        pvalues = model.pvalues.iloc[1:]\n",
        "        worst_pval = pvalues.max()  # Null if pvalues is empty\n",
        "        if worst_pval > threshold_out:\n",
        "            changed = True\n",
        "            worst_feature = pvalues.idxmax()\n",
        "            included.remove(worst_feature)\n",
        "            if verbose:\n",
        "                print(f'Drop {worst_feature:30} with p-value {worst_pval:.6}')\n",
        "        if not changed:\n",
        "            break\n",
        "    return included"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "itonA5mFw8vE"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = stepwise_selection(X_train, y_train)\n",
        "print('recursos resultantes:')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDS2qI3hxE6o",
        "outputId": "8e942dd1-514c-4987-bc1b-70a7dbb026df"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add  tempo_emprego                  with p-value 0.0\n",
            "Add  qt_pessoas_residencia          with p-value 1.75831e-12\n",
            "Add  qtd_filhos                     with p-value 0.000908728\n",
            "Add  idade                          with p-value 0.000251356\n",
            "recursos resultantes:\n",
            "['tempo_emprego', 'qt_pessoas_residencia', 'qtd_filhos', 'idade']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar e treinar o modelo com as variáveis selecionadas\n",
        "X_train_stepwise = X_train[result]\n",
        "X_test_stepwise = X_test[result]\n",
        "\n",
        "model = sm.OLS(y_train, sm.add_constant(X_train_stepwise)).fit()\n",
        "\n",
        "# Fazer previsões na base de teste\n",
        "y_pred_stepwise = model.predict(sm.add_constant(X_test_stepwise))\n",
        "\n",
        "# Avaliar o modelo com o R²\n",
        "r2_stepwise = r2_score(y_test, y_pred_stepwise)\n",
        "\n",
        "print(f'R² Score com Stepwise: {r2_stepwise}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9BKVs0CxcBJ",
        "outputId": "12808f58-167c-4a17-bc4a-85dcbffaf83e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score com Stepwise: 0.1546641272297311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O modelo Ridge com alpha = 0.01 teve o melhor desempenho, com um\n",
        "𝑅\n",
        "2\n",
        " de 0.268. Ele foi melhor que os modelos LASSO e Stepwise."
      ],
      "metadata": {
        "id": "v1YwIzh0yS3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge_model = Ridge(alpha=0.01)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "ridge_coefs = pd.Series(ridge_model.coef_, index=X_train.columns)\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "lasso_model = Lasso(alpha=0)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "lasso_coefs = pd.Series(lasso_model.coef_, index=X_train.columns)\n",
        "\n",
        "import statsmodels.api as sm\n",
        "stepwise_features = stepwise_selection(X_train, y_train)\n",
        "X_train_stepwise = X_train[stepwise_features]\n",
        "X_test_stepwise = X_test[stepwise_features]\n",
        "stepwise_model = sm.OLS(y_train, sm.add_constant(X_train_stepwise)).fit()\n",
        "stepwise_coefs = stepwise_model.params\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LsHgl-AyWoz",
        "outputId": "f2f2d26b-54f4-4351-bbc0-cc82a45e7020"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1473: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  return fit_method(estimator, *args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.855e+11, tolerance: 7.723e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add  tempo_emprego                  with p-value 0.0\n",
            "Add  qt_pessoas_residencia          with p-value 1.75831e-12\n",
            "Add  qtd_filhos                     with p-value 0.000908728\n",
            "Add  idade                          with p-value 0.000251356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Ridge': ridge_coefs,\n",
        "    'LASSO': lasso_coefs,\n",
        "    'Stepwise': stepwise_coefs\n",
        "})\n",
        "print(comparison_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnGZGJ0sy4Fc",
        "outputId": "0dc8f9b4-2eaa-4eb9-a556-16ad7ff384c6"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     Ridge        LASSO     Stepwise\n",
            "Unnamed: 0                       -0.204916    -0.177992          NaN\n",
            "const                                  NaN          NaN   864.470144\n",
            "data_ref_2015-02-01             225.258953   194.765104          NaN\n",
            "data_ref_2015-03-01             700.651233   643.288287          NaN\n",
            "data_ref_2015-04-01            1178.874929  1094.720252          NaN\n",
            "data_ref_2015-05-01            1049.641831   938.430222          NaN\n",
            "data_ref_2015-06-01            1608.678167  1470.825072          NaN\n",
            "data_ref_2015-07-01            1499.102341  1334.004416          NaN\n",
            "data_ref_2015-08-01            1706.859263  1515.132048          NaN\n",
            "data_ref_2015-09-01            1928.064140  1709.436748          NaN\n",
            "data_ref_2015-10-01            1944.179600  1698.684154          NaN\n",
            "data_ref_2015-11-01            2373.367392  2100.825708          NaN\n",
            "data_ref_2015-12-01            3054.730604  2755.461903          NaN\n",
            "data_ref_2016-01-01            2782.695909  2456.349932          NaN\n",
            "data_ref_2016-02-01            2653.572355  2300.455855          NaN\n",
            "data_ref_2016-03-01            2959.071562  2579.290377          NaN\n",
            "educacao_Pós graduação         1002.933178  1002.050343          NaN\n",
            "educacao_Secundário              98.104470    98.632444          NaN\n",
            "educacao_Superior completo      737.030582   737.719918          NaN\n",
            "educacao_Superior incompleto   -405.118015  -404.098903          NaN\n",
            "estado_civil_Separado           999.732438   990.713893          NaN\n",
            "estado_civil_Solteiro           637.597692   628.887819          NaN\n",
            "estado_civil_União             -305.297039  -305.398954          NaN\n",
            "estado_civil_Viúvo             1080.761103  1072.129692          NaN\n",
            "id_cliente                        0.007752     0.007728          NaN\n",
            "idade                            35.273118    35.283850   -25.367754\n",
            "posse_de_imovel                 299.754932   299.942956          NaN\n",
            "posse_de_veiculo                  3.964327     4.110575          NaN\n",
            "qt_pessoas_residencia          1130.791119  1122.026650  1045.194616\n",
            "qtd_filhos                    -1034.125636 -1025.355352  -809.065618\n",
            "sexo_M                         5844.475210  5844.275517          NaN\n",
            "tempo_emprego                   551.777305   551.778925   529.253986\n",
            "tipo_renda_Bolsista           -1003.231038 -1002.130481          NaN\n",
            "tipo_renda_Empresário           868.270457   868.253957          NaN\n",
            "tipo_renda_Pensionista         -475.424029  -475.783444          NaN\n",
            "tipo_renda_Servidor público      87.806986    87.500806          NaN\n",
            "tipo_residencia_Casa           -160.289143  -160.999165          NaN\n",
            "tipo_residencia_Com os pais    -186.058022  -187.083149          NaN\n",
            "tipo_residencia_Comunitário    -337.662845  -339.352154          NaN\n",
            "tipo_residencia_Estúdio         874.612826   872.822099          NaN\n",
            "tipo_residencia_Governamental   610.038384   610.124824          NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliação dos Modelos\n",
        "\n",
        "Modelo Ridge:\n",
        "\n",
        "Maior\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " : 0,2680\n",
        "Usa regularização para evitar overfitting, distribuindo os pesos suavemente entre as variáveis.\n",
        "\n",
        "Modelo LASSO:\n",
        "\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  um pouco menor que o Ridge: 0,2680\n",
        "Também usa regularização, mas zera muitos coeficientes, tornando o modelo mais simples.\n",
        "\n",
        "Modelo Stepwise:\n",
        "\n",
        "Menor\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " : 0,1547\n",
        "Seleciona variáveis passo a passo, mas é menos robusto que os métodos de regularização.\n",
        "\n",
        "Conclusão\n",
        "\n",
        "O modelo Ridge (\n",
        "𝛼\n",
        "=\n",
        "0\n",
        ",\n",
        "01\n",
        "α=0,01) teve o melhor desempenho (\n",
        "𝑅\n",
        "2\n",
        "=\n",
        "0\n",
        ",\n",
        "2680\n",
        "R\n",
        "2\n",
        " =0,2680), equilibrando bem o uso das variáveis sem zerar muitos coeficientes. Por isso, é o mais indicado para este conjunto de dados."
      ],
      "metadata": {
        "id": "BSx6EsIOzKW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "\n",
        "# Aplicar transformações logarítmicas onde faz sentido\n",
        "X_train['log_tempo_emprego'] = np.log1p(X_train['tempo_emprego'])\n",
        "X_test['log_tempo_emprego'] = np.log1p(X_test['tempo_emprego'])\n",
        "\n",
        "# Criar variáveis interativas e polinomiais\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Normalizar as variáveis\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_poly)\n",
        "X_test_scaled = scaler.transform(X_test_poly)\n",
        "\n",
        "# Treinar e avaliar o modelo Ridge com as novas transformações\n",
        "ridge_model = Ridge(alpha=0.01)\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
        "\n",
        "# Avaliar o modelo com o R²\n",
        "r2_transformed_ridge = r2_score(y_test, y_pred_ridge)\n",
        "print(f'R² Score com Ridge (transformado): {r2_transformed_ridge}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugKaLI2G1p9j",
        "outputId": "79d8baaa-a942-4c7e-b797-d3fb54965e54"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score com Ridge (transformado): 0.3074348447081805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A transformação e a normalização das variáveis resultaram em uma melhora significativa no\n",
        "𝑅\n",
        "2\n",
        ". O modelo Ridge com as transformações aplicadas conseguiu um\n",
        "𝑅\n",
        "2\n",
        " de 0.307, o que é um avanço considerável em comparação com os valores anteriores.\n",
        "\n",
        "Comparação Final dos Resultados\n",
        "Modelo Ridge (sem transformações):\n",
        "\n",
        "Melhor\n",
        "𝑅\n",
        "2\n",
        ": 0.268\n",
        "\n",
        "Modelo LASSO:\n",
        "\n",
        "Melhor\n",
        "𝑅\n",
        "2\n",
        ": 0.268\n",
        "\n",
        "Modelo Stepwise:\n",
        "\n",
        "𝑅\n",
        "2\n",
        ": 0.155\n",
        "\n",
        "Modelo Ridge (com transformações):\n",
        "\n",
        "Melhor\n",
        "𝑅\n",
        "2\n",
        ": 0.307\n",
        "\n",
        "Conclusão\n",
        "Com base nos resultados, o modelo Ridge com transformações apresentou o melhor desempenho, com um\n",
        "𝑅\n",
        "2\n",
        " de 0.307. Isso demonstra que aplicar transformações logarítmicas e gerar interações entre as variáveis pode ajudar a melhorar a precisão do modelo."
      ],
      "metadata": {
        "id": "8szT6XoB2Dac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "\n",
        "# Criar e treinar a árvore de regressão\n",
        "tree_model = DecisionTreeRegressor(random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões na base de teste\n",
        "y_pred_tree = tree_model.predict(X_test)\n",
        "\n",
        "# Avaliar o modelo com o R²\n",
        "r2_tree = r2_score(y_test, y_pred_tree)\n",
        "print(f'R² Score com Árvore de Regressão: {r2_tree}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1iRhey02fuq",
        "outputId": "94bc8161-0bbb-4bcf-db73-5ed7df428208"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R² Score com Árvore de Regressão: 0.11752192183511612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparando os resultados, observamos que a árvore de regressão não superou os outros modelos, apresentando um\n",
        "𝑅\n",
        "2\n",
        " de aproximadamente 0.118. Aqui estão os resultados finais:\n",
        "\n",
        "Comparação dos Resultados\n",
        "Modelo Ridge (sem transformações):\n",
        "\n",
        "Melhor\n",
        "𝑅\n",
        "2\n",
        ": 0.268\n",
        "\n",
        "Modelo LASSO:\n",
        "\n",
        "Melhor\n",
        "𝑅\n",
        "2\n",
        ": 0.268\n",
        "\n",
        "Modelo Stepwise:\n",
        "\n",
        "𝑅\n",
        "2\n",
        ": 0.155\n",
        "\n",
        "Modelo Ridge (com transformações):\n",
        "\n",
        "Melhor\n",
        "𝑅\n",
        "2\n",
        ": 0.307\n",
        "\n",
        "Árvore de Regressão:\n",
        "\n",
        "𝑅\n",
        "2\n",
        ": 0.118\n",
        "\n",
        "Conclusão\n",
        "O modelo Ridge com transformações permanece como o melhor, com um\n",
        "𝑅\n",
        "2\n",
        " de 0.307, superando os outros métodos, incluindo a árvore de regressão."
      ],
      "metadata": {
        "id": "z_b8a2RL2wx7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}